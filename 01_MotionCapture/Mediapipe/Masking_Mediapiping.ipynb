{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ae041d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to build a motion tracking pipeline: Using MediaPipe with optional masking\"\n",
    "authors: Wim Pouw, adapted by Šárka Kadavá\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93545df",
   "metadata": {},
   "source": [
    "## Info documents\n",
    "\n",
    "This python notebook runs you through the procedure of taking videos as inputs with a single person in the video, and outputting the 1 outputs of the kinematic timeseries, and optionally masking video with facial, hand, and arm kinematics ovelayen.\n",
    "\n",
    "The masked-piper tool is a simple but effective modification of the the Holistic Tracking by Google's Mediapipe so that we can use it as a CPU-based light weigth tool to mask your video data while maintaining background information, and also preserving information about body kinematics. \n",
    "\n",
    "## Additional information backbone of the tool (Mediapipe Holistic Tracking)\n",
    "https://google.github.io/mediapipe/solutions/holistic.html\n",
    "\n",
    "## Citation of mediapipe\n",
    "citation: Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., ... & Grundmann, M. (2019). Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172.\n",
    "\n",
    "## Citation of masked piper\n",
    "* citation: Owoyele, B., Trujillo, J., De Melo, G., & Pouw, W. (2022). Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information. SoftwareX, 20, 101236. \n",
    "* Original Repo: https://github.com/WimPouw/TowardsMultimodalOpenScience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2fc03",
   "metadata": {},
   "source": [
    "The first thing we **always** want to do when we open a notebook, is to setup our environment - that means importing packages, setting up folder dependencies etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f423ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "import glob\n",
    "\n",
    "curfolder = os.getcwd() #get the current working directory\n",
    "\n",
    "inputfolder = curfolder + \"/Input_Videos/\" #set the path to the input videos\n",
    "videofiles = glob.glob(inputfolder + \"*.avi\") #get all the mp4 files in the input folder\n",
    "\n",
    "outputf_mask = curfolder + \"/Output_Videos/\" #video with skeleton/mask\n",
    "#create the output folder if it does not exist\n",
    "if not os.path.exists(outputf_mask):\n",
    "    os.makedirs(outputf_mask)\n",
    "outtputf_ts = curfolder + \"/Output_TimeSeries/\" #time series output folder\n",
    "#create the output folder if it does not exist\n",
    "if not os.path.exists(outtputf_ts):\n",
    "    os.makedirs(outtputf_ts)\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outtputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_mask))\n",
    "print(\"\\n The following video(s) will be processed for masking: \")\n",
    "print(videofiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806be3cc",
   "metadata": {},
   "source": [
    "Now, we can start to prepare mediapipe by loading in functions and setting the keypoinnts of interest.\n",
    "\n",
    "Unless you are a developer and want to modify the code to utilize some special functions, this code can most likely remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06712ae9",
   "metadata": {},
   "source": [
    "## Main procedure Masked-Piper\n",
    "The following chunk of code loops through all the videos you have loaded into the input folder, then assess each frame for body poses, extract kinematic info, masks the body in a new frame that keeps the background, projects the kinematic info on the mask, and stores the kinematic info for that frame into the time series .csv for the hand + body + face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d95d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do you want to apply masking?\n",
    "masking = True\n",
    "\n",
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in videofiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(videofiles.index(vidf)) + \" of \" + str(len(videofiles)) + \" videos in total\")\n",
    "    \n",
    "    # Capture the video and check video settings\n",
    "    videoname = vidf.split(\"\\\\\")[-1]\n",
    "    #videoname2 = videoname.replace(\"mp4\", \"avi\")\n",
    "    videoloc = vidf\n",
    "    capture = cv2.VideoCapture(videoloc)  # load the video capture\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH) #check frame width 1280 \n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT) #check frame height 720\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)   #fps = frames per second 30\n",
    "    #print(frameWidth, frameHeight, samplerate)\n",
    "\n",
    "    # Create an empty video file to project the pose tracking on\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # for different video formats you could use e.g., *'XVID'\n",
    "    #print(fourcc)\n",
    "    out = cv2.VideoWriter(outputf_mask + videoname, fourcc, fps=samplerate, \n",
    "                          frameSize=(int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    # Initialize Mediapipe Holistic\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]  # These are the time series objects starting with column names initialized above\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    # Adding real-world landmarks\n",
    "    tsbody_world = [markerxyzbody]  # For world landmarks (3D coordinates)\n",
    "    tsface_world = [markerxyzface]  # For normalized 3D face landmarks\n",
    "    tshands_world = [markerxyzhands]  # For normalized 3D hand landmarks\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read()  # read frame\n",
    "            if ret == True:  # if there is a frame\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # ensure the image is in RGB format\n",
    "                results = holistic.process(image)  # apply Mediapipe holistic processing\n",
    "                \n",
    "                h, w, c = image.shape\n",
    "                if results.face_landmarks or results.pose_landmarks or results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                    if masking == False:\n",
    "                        original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    elif masking == True:\n",
    "                        # Masking logic here (if needed)\n",
    "                        original_image = np.concatenate([image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        mask_img = np.zeros_like(image, dtype=np.uint8) #set up basic mask image\n",
    "                        mask_img[:, :] = (255,255,255) #set up basic mask image\n",
    "                        segm_2class = 0.2 + 0.8 * results.segmentation_mask #set up a segmentation of the results of mediapipe\n",
    "                        segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2) #set up a segmentation of the results of mediapipe\n",
    "                        annotated_image = mask_img * segm_2class * (1 - segm_2class) #take the basic mask image and make a sillhouette mask\n",
    "                        # append Alpha channel to sillhouetted mask so that we can overlay it to the original image\n",
    "                        mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        # Zero background where we want to overlay\n",
    "                        original_image[mask==0]=0 #for the original image we are going to set everything at zero for places where the mask has to go\n",
    "                        original_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Draw left hand, right hand, face, and body pose landmarks\n",
    "                    mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        original_image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        original_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                    \n",
    "                    # Save pose landmarks (2D image-based)\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                    # Save pose world landmarks (3D coordinates in meters)\n",
    "                    if results.pose_world_landmarks:\n",
    "                        samplebody_world = listpostions(results.pose_world_landmarks)\n",
    "                        samplebody_world.insert(0, time)\n",
    "                        tsbody_world.append(samplebody_world)\n",
    "\n",
    "                    # Save face and hand landmarks (as normalized 3D landmarks)\n",
    "                    if results.face_landmarks:\n",
    "                        sampleface_world = listpostions(results.face_landmarks)\n",
    "                        sampleface_world.insert(0, time)\n",
    "                        tsface_world.append(sampleface_world)\n",
    "\n",
    "                    if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                        samplehands_world = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                        samplehands_world.insert(0, time)\n",
    "                        tshands_world.append(samplehands_world)\n",
    "\n",
    "                else:\n",
    "                    # If no landmarks detected, append NaNs\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "\n",
    "                    # Append NaNs for world coordinates as well\n",
    "                    samplebody_world = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplebody_world.insert(0, time)\n",
    "                    tsbody_world.append(samplebody_world)\n",
    "\n",
    "                    sampleface_world = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    sampleface_world.insert(0, time)\n",
    "                    tsface_world.append(sampleface_world)\n",
    "\n",
    "                    samplehands_world = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    samplehands_world.insert(0, time)\n",
    "                    tshands_world.append(samplehands_world)\n",
    "\n",
    "                # Show the video as we process\n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)  # save the frame to the new masked video\n",
    "                time += (1000 / samplerate)  # update the time variable for the next frame\n",
    "            \n",
    "            if cv2.waitKey(1) == 27:  # allow the use of ESCAPE to break the loop\n",
    "                break\n",
    "            if ret == False:  # if there are no more frames, break the loop\n",
    "                break\n",
    "\n",
    "    # Once done, de-initialize all processes\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save CSV data for body, face, and hands\n",
    "    filebody = open(outtputf_ts + videoname + '_body.csv', 'w+', newline='')\n",
    "    with filebody:\n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "    \n",
    "    filehands = open(outtputf_ts + videoname + '_hands.csv', 'w+', newline='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "    \n",
    "    fileface = open(outtputf_ts + videoname + '_face.csv', 'w+', newline='')\n",
    "    with fileface:\n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "    # Save world coordinates (in meters) to CSV for body, face, and hands\n",
    "    filebody_world = open(outtputf_ts + videoname + '_body_world.csv', 'w+', newline='')\n",
    "    with filebody_world:\n",
    "        write = csv.writer(filebody_world)\n",
    "        write.writerows(tsbody_world)\n",
    "\n",
    "    fileface_world = open(outtputf_ts + videoname + '_face_world.csv', 'w+', newline='')\n",
    "    with fileface_world:\n",
    "        write = csv.writer(fileface_world)\n",
    "        write.writerows(tsface_world)\n",
    "\n",
    "    filehands_world = open(outtputf_ts + videoname + '_hands_world.csv', 'w+', newline='')\n",
    "    with filehands_world:\n",
    "        write = csv.writer(filehands_world)\n",
    "        write.writerows(tshands_world)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233eb839",
   "metadata": {},
   "source": [
    "## Quality check\n",
    "\n",
    "One thing is really really important - we must resist the temptation to treat code as a black box. It can be code that you are reproducing from the most-cited paper, and yet it can go wrong in any step of the way.\n",
    "\n",
    "This is even more crucial for pipeline like MediaPipe where we fit some body model onto a video but do not, for example, calibrate the cameras. All keypoints are mere estimation - and this can be tricky especially for the depth (z-)dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef03b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import Video\n",
    "\n",
    "# 3D tracking checking\n",
    "outputcsv = glob.glob(outtputf_ts + '*.csv')\n",
    "fileexample = outputcsv[1]\n",
    "\n",
    "MT_tracking = pd.read_csv(fileexample)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define the number of frames\n",
    "num_frames = len(MT_tracking)\n",
    "\n",
    "# Define the scatter plot\n",
    "scatter = ax.scatter([], [], [], marker='o')\n",
    "\n",
    "# Update function for animation\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Z Label')\n",
    "    ax.set_zlabel('Y Label')\n",
    "    ax.set_title('3D Animation')\n",
    "    \n",
    "    # Set the limits of the axes based on the maximum values\n",
    "    ax.set_xlim3d(0, 1)\n",
    "    ax.set_ylim3d(20, 0)\n",
    "    ax.set_zlim3d(0, 1) #we need to flip the vertical\n",
    "    \n",
    "    # Plot the data for the current frame\n",
    "    frame_data = MT_tracking.iloc[frame]\n",
    "    frame_data_body = MT_tracking.iloc[frame]\n",
    "    x = frame_data.filter(like='X_')\n",
    "    x_body = frame_data_body.filter(like='X_')\n",
    "    y = frame_data.filter(like='Z_')\n",
    "    y_body = frame_data_body.filter(like='Z_')\n",
    "    z = frame_data.filter(like='Y_')\n",
    "    z_body = frame_data_body.filter(like='Y_')\n",
    "    # create scatter that has body in blue, and hands in red\n",
    "    #scatter = ax.scatter(x, y, z, color='blue')\n",
    "    scatter = ax.scatter(x_body, y_body, z_body*-1, color='red')\n",
    "\n",
    "    return scatter\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, update, frames=num_frames, interval=1000/60)\n",
    "\n",
    "# Close the figure to prevent displaying the first frame\n",
    "plt.close(fig)\n",
    "\n",
    "# Save the animation as a video\n",
    "ani.save('3D_animation.mp4', writer='ffmpeg')\n",
    "\n",
    "# Display the animation\n",
    "video_path = '3D_animation.mp4'\n",
    "Video(video_path, embed=True, width=800)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38538410",
   "metadata": {},
   "source": [
    "We can also check the video directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "# Create a temporary folder\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Get the same video\n",
    "videos = glob.glob(outputf_mask + '*.avi')\n",
    "videoexample = videos[0]\n",
    "\n",
    "mp4_path = os.path.join(temp_dir, \"converted_rawtrackingvideo.mp4\")  # Save MP4 in temp folder\n",
    "\n",
    "\n",
    "# Convert AVI to MP4 with video and audio\n",
    "if not os.path.exists(mp4_path):\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-i\", videoexample, \n",
    "        \"-vcodec\", \"libx264\", \"-acodec\", \"aac\", \"-b:a\", \"192k\",  # Ensure audio is included\n",
    "        \"-crf\", \"23\", \"-preset\", \"fast\", mp4_path\n",
    "    ])\n",
    "\n",
    "# Display the video inside Jupyter Notebook\n",
    "Video(mp4_path, embed=True, width=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
